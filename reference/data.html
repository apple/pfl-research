<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="article:modified_time" content="2025-05-28T17:07:54+00:00" /><link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Exception" href="exception.html" /><link rel="prev" title="Context" href="context.html" />

    <!-- Generated with Sphinx 7.2.6 and Furo 2023.09.10 -->
        <title>Data - pfl 0.3.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=135e06be" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=36a5483c" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">pfl 0.3.1 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  
  <span class="sidebar-brand-text">pfl 0.3.1 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Getting Started:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../guides/fl_introduction.html">Federated learning with pfl</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guides/simulation_distributed.html">Fast distributed simulations</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../support/contributing.html">Contributing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="algorithm.html">Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="aggregate.html">Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="aggregate.html#aggregator">Aggregator</a></li>
<li class="toctree-l1"><a class="reference internal" href="aggregate.html#module-pfl.aggregate.data_transport">Data transport</a></li>
<li class="toctree-l1"><a class="reference internal" href="aggregate.html#module-pfl.aggregate.weighting">Weighting</a></li>
<li class="toctree-l1"><a class="reference internal" href="callback.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="common_types.html">Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="context.html">Context</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="exception.html">Exception</a></li>
<li class="toctree-l1"><a class="reference internal" href="hyperparam.html">Hyperparameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="model.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="postprocessor.html">Postprocessor</a></li>
<li class="toctree-l1"><a class="reference internal" href="privacy.html">Differential privacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="stats.html">Training statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="tree.html">Gradient boosted decision trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="environment_variables.html">Environment variables</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="internal/index.html">Internal API</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Internal API</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="internal/bisect.html">Bisect</a></li>
<li class="toctree-l2"><a class="reference internal" href="internal/bridge.html">Bridges</a></li>
<li class="toctree-l2"><a class="reference internal" href="internal/distribution.html">Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="internal/ops.html">Ops</a></li>
<li class="toctree-l2"><a class="reference internal" href="internal/platform.html">Platform</a></li>
<li class="toctree-l2"><a class="reference internal" href="internal/privacy_loss_bound.html">Privacy loss bound</a></li>
<li class="toctree-l2"><a class="reference internal" href="internal/tree.html">Tree</a></li>
</ul>
</li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="data">
<h1>Data<a class="headerlink" href="#data" title="Link to this heading">#</a></h1>
<section id="module-pfl.data.dataset">
<span id="user-dataset"></span><h2>User dataset<a class="headerlink" href="#module-pfl.data.dataset" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="pfl.data.dataset.AbstractDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pfl.data.dataset.</span></span><span class="sig-name descname"><span class="pre">AbstractDataset</span></span><a class="headerlink" href="#pfl.data.dataset.AbstractDataset" title="Link to this definition">#</a></dt>
<dd><p>Base class for user dataset representations.</p>
<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.dataset.AbstractDataset.split">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">split</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fraction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_train_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_val_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.dataset.AbstractDataset.split" title="Link to this definition">#</a></dt>
<dd><p>Split the dataset into two smaller disjoint datasets. Used by
algorithms that require both a train and val dataset for each user.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fraction</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]) – The fraction of data to split at. Defaults to <code class="docutils literal notranslate"><span class="pre">0.8</span></code></p></li>
<li><p><strong>min_train_size</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – The minimum number of samples for the train partition after
the split. If the dataset size is <code class="docutils literal notranslate"><span class="pre">4</span></code> and split is done with the
parameters <code class="docutils literal notranslate"><span class="pre">fraction=0.1</span></code> and <code class="docutils literal notranslate"><span class="pre">min_train_size=1</span></code>, the split
would result in a train set of size <code class="docutils literal notranslate"><span class="pre">0</span></code>, but <code class="docutils literal notranslate"><span class="pre">min_train_size</span></code>
overrides that to <code class="docutils literal notranslate"><span class="pre">1</span></code>.
Defaults to <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p><strong>min_val_size</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Same as <code class="docutils literal notranslate"><span class="pre">min_train_size</span></code>, but for the val partition. An error
is thrown if dataset length before split is less than
<code class="docutils literal notranslate"><span class="pre">min_train_size+min_val_size</span></code>.
Defaults to <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<a class="reference internal" href="#pfl.data.dataset.AbstractDataset" title="pfl.data.dataset.AbstractDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">AbstractDataset</span></code></a>, <a class="reference internal" href="#pfl.data.dataset.AbstractDataset" title="pfl.data.dataset.AbstractDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">AbstractDataset</span></code></a>]</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A tuple <code class="docutils literal notranslate"><span class="pre">(left_dataset,</span> <span class="pre">right_dataset)</span></code>, where <code class="docutils literal notranslate"><span class="pre">left_dataset</span></code>
is a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> with a fraction of <code class="docutils literal notranslate"><span class="pre">fraction</span></code> of the data
and <code class="docutils literal notranslate"><span class="pre">right_dataset</span></code> is a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> with a fraction of
<code class="docutils literal notranslate"><span class="pre">1-fraction</span></code> of the data.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.dataset.AbstractDataset.get_worker_partition">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">get_worker_partition</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.dataset.AbstractDataset.get_worker_partition" title="Link to this definition">#</a></dt>
<dd><p>Partition the dataset among active workers in multi-worker training and
return a unique partition for the current worker.
If multi-worker training is not in use, this method just returns the
identity because the partitioning is 1 set with the full data.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#pfl.data.dataset.AbstractDataset" title="pfl.data.dataset.AbstractDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">AbstractDataset</span></code></a></p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A subset of this dataset, unique to the current worker. All worker
partitions add up to the original dataset.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="pfl.data.dataset.Dataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pfl.data.dataset.</span></span><span class="sig-name descname"><span class="pre">Dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">raw_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metadata</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_indices</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.dataset.Dataset" title="Link to this definition">#</a></dt>
<dd><p>A representation of a flat (user) dataset.
Use only for smaller (user) datasets that fit in memory.</p>
<p>If using PyTorch or TF tensors, use datasets in
<a class="reference internal" href="#module-pfl.data.tensorflow" title="pfl.data.tensorflow"><code class="xref py py-mod docutils literal notranslate"><span class="pre">pfl.data.tensorflow</span></code></a> and <a class="reference internal" href="#module-pfl.data.pytorch" title="pfl.data.pytorch"><code class="xref py py-mod docutils literal notranslate"><span class="pre">pfl.data.pytorch</span></code></a>
instead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>raw_data</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">...</span></code>]) – A tuple of data inputs in the order they are specified by the model.
The “order” for some common models is given by:
* Tensorflow - First entry is a tuple of features (input to model),
second entry is a tuple of labels (input to loss/metrics).
* PyTorch - a tuple of tensors, unrolled to <code class="docutils literal notranslate"><span class="pre">model.loss</span></code> and
<code class="docutils literal notranslate"><span class="pre">model.metrics</span></code>.</p></li>
<li><p><strong>user_id</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – (Optional) String user identifier. Only needed if you are using
algorithms or other components that explicitly make use of user IDs.
You will notice if that is the case from errors that say user IDs
must be defined.</p></li>
<li><p><strong>metadata</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – (Optional) Store additional data about the user. Can be retrieved
later by the algorithm.</p></li>
<li><p><strong>train_kwargs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – A dictionary of any additional training parameters to unpack in the
training call of the deep learning framework.</p></li>
<li><p><strong>eval_kwargs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – A dictionary of any additional evaluation parameters to unpack in the
evaluation call of the deep learning framework.</p></li>
<li><p><strong>val_indices</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]) – <dl class="simple">
<dt>TODO: <a class="reference external" href="rdar://115345691">rdar://115345691</a> (enable val_indices feature in user</dt><dd><p>Dataset). this parameter is not actually used right now. Meanwhile,
use parameters of <code class="docutils literal notranslate"><span class="pre">split</span></code> instead.</p>
</dd>
</dl>
<p>A list of datapoint indices specifying which datapoints in
<code class="docutils literal notranslate"><span class="pre">raw_data</span></code> to split into the val partition when calling
<code class="docutils literal notranslate"><span class="pre">split</span></code> on this user dataset. This parameter is optional but useful
when you have an explicit set of val datapoints for this user. As
noted above, this parameter is currently not used.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Splitting a user’s dataset is only done by algorithms
that require a local val dataset for measuring the generalized
performance after training, e.g. meta-learning or personalization
via fine-tuning the model locally on each device.</p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.dataset.Dataset.split">
<span class="sig-name descname"><span class="pre">split</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fraction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_train_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_val_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.dataset.Dataset.split" title="Link to this definition">#</a></dt>
<dd><p>Split the dataset into two smaller disjoint datasets. Used by
algorithms that require both a train and val dataset for each user.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fraction</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]) – The fraction of data to split at. Defaults to <code class="docutils literal notranslate"><span class="pre">0.8</span></code></p></li>
<li><p><strong>min_train_size</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – The minimum number of samples for the train partition after
the split. If the dataset size is <code class="docutils literal notranslate"><span class="pre">4</span></code> and split is done with the
parameters <code class="docutils literal notranslate"><span class="pre">fraction=0.1</span></code> and <code class="docutils literal notranslate"><span class="pre">min_train_size=1</span></code>, the split
would result in a train set of size <code class="docutils literal notranslate"><span class="pre">0</span></code>, but <code class="docutils literal notranslate"><span class="pre">min_train_size</span></code>
overrides that to <code class="docutils literal notranslate"><span class="pre">1</span></code>.
Defaults to <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p><strong>min_val_size</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Same as <code class="docutils literal notranslate"><span class="pre">min_train_size</span></code>, but for the val partition. An error
is thrown if dataset length before split is less than
<code class="docutils literal notranslate"><span class="pre">min_train_size+min_val_size</span></code>.
Defaults to <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<a class="reference internal" href="#pfl.data.dataset.Dataset" title="pfl.data.dataset.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a>, <a class="reference internal" href="#pfl.data.dataset.Dataset" title="pfl.data.dataset.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a>]</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A tuple <code class="docutils literal notranslate"><span class="pre">(left_dataset,</span> <span class="pre">right_dataset)</span></code>, where <code class="docutils literal notranslate"><span class="pre">left_dataset</span></code>
is a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> with a fraction of <code class="docutils literal notranslate"><span class="pre">fraction</span></code> of the data
and <code class="docutils literal notranslate"><span class="pre">right_dataset</span></code> is a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> with a fraction of
<code class="docutils literal notranslate"><span class="pre">1-fraction</span></code> of the data.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.dataset.Dataset.get_worker_partition">
<span class="sig-name descname"><span class="pre">get_worker_partition</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.dataset.Dataset.get_worker_partition" title="Link to this definition">#</a></dt>
<dd><p>Partition the dataset among active workers in multi-worker training and
return a unique partition for the current worker.
If multi-worker training is not in use, this method just returns the
identity because the partitioning is 1 set with the full data.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#pfl.data.dataset.AbstractDataset" title="pfl.data.dataset.AbstractDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">AbstractDataset</span></code></a></p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A subset of this dataset, unique to the current worker. All worker
partitions add up to the original dataset.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="pfl.data.dataset.TabularDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pfl.data.dataset.</span></span><span class="sig-name descname"><span class="pre">TabularDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metadata</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_indices</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.dataset.TabularDataset" title="Link to this definition">#</a></dt>
<dd><p>A dataset comprising tabular dataset: (features, labels).
See <a class="reference internal" href="#pfl.data.dataset.Dataset" title="pfl.data.dataset.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a> for more information about
parameters.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="pfl.data.dataset.DatasetSplit">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pfl.data.dataset.</span></span><span class="sig-name descname"><span class="pre">DatasetSplit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_dataset</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.dataset.DatasetSplit" title="Link to this definition">#</a></dt>
<dd><p>Decorates the dataset split into predefined <code class="docutils literal notranslate"><span class="pre">train_dataset</span></code> and <code class="docutils literal notranslate"><span class="pre">val_dataset</span></code>.
This means that when using an instance of this class without splitting has
the same behavior as <code class="docutils literal notranslate"><span class="pre">train_dataset</span></code> except <code class="docutils literal notranslate"><span class="pre">iter</span></code> is only implemented
for the two datasets from the split.</p>
<p>This class is useful when using algorithms that require a local train-val
split, e.g. meta-learning, and you want to pre-define how the split should
be done.</p>
<dl class="py property">
<dt class="sig sig-object py" id="pfl.data.dataset.DatasetSplit.raw_data">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">raw_data</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#pfl.data.dataset.DatasetSplit.raw_data" title="Link to this definition">#</a></dt>
<dd><p>The raw data of the train dataset.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="pfl.data.dataset.DatasetSplit.train_kwargs">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">train_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#pfl.data.dataset.DatasetSplit.train_kwargs" title="Link to this definition">#</a></dt>
<dd><p>Additional training arguments of the train dataset.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="pfl.data.dataset.DatasetSplit.eval_kwargs">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">eval_kwargs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#pfl.data.dataset.DatasetSplit.eval_kwargs" title="Link to this definition">#</a></dt>
<dd><p>Additional evaluation arguments of the train dataset.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.dataset.DatasetSplit.split">
<span class="sig-name descname"><span class="pre">split</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fraction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_train_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_val_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.dataset.DatasetSplit.split" title="Link to this definition">#</a></dt>
<dd><p>Split the dataset by returning the train and val <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> previously
specified in the constructor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fraction</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]) – Has no effect since the split is determined when initializing.</p></li>
<li><p><strong>min_train_size</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Has no effect since the split is determined when initializing.</p></li>
<li><p><strong>min_val_size</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Has no effect since the split is determined when initializing.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<a class="reference internal" href="#pfl.data.dataset.AbstractDataset" title="pfl.data.dataset.AbstractDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">AbstractDataset</span></code></a>, <a class="reference internal" href="#pfl.data.dataset.AbstractDataset" title="pfl.data.dataset.AbstractDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">AbstractDataset</span></code></a>]</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A tuple with the user datasets <code class="docutils literal notranslate"><span class="pre">train_dataset</span></code> and <code class="docutils literal notranslate"><span class="pre">val_dataset</span></code>
previously specified in the constructor.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.dataset.DatasetSplit.get_worker_partition">
<span class="sig-name descname"><span class="pre">get_worker_partition</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.dataset.DatasetSplit.get_worker_partition" title="Link to this definition">#</a></dt>
<dd><p>Partition the dataset among active workers in multi-worker training and
return a unique partition for the current worker.
If multi-worker training is not in use, this method just returns the
identity because the partitioning is 1 set with the full data.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#pfl.data.dataset.AbstractDataset" title="pfl.data.dataset.AbstractDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">AbstractDataset</span></code></a></p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A subset of this dataset, unique to the current worker. All worker
partitions add up to the original dataset.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class" id="module-pfl.data.tensorflow">
<dt class="sig sig-object py" id="pfl.data.tensorflow.TFTensorDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pfl.data.tensorflow.</span></span><span class="sig-name descname"><span class="pre">TFTensorDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metadata</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_indices</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.tensorflow.TFTensorDataset" title="Link to this definition">#</a></dt>
<dd><p>In-memory TensorFlow tensors representing a user dataset.
See <a class="reference internal" href="#pfl.data.dataset.Dataset" title="pfl.data.dataset.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a> for more information about
the class and its parameters.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>features</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">...</span></code>]]) – A single <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code> or a tuple of <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code>. Should be valid
input to your Tensorflow model’s <code class="docutils literal notranslate"><span class="pre">call</span></code> method.</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">...</span></code>]]) – A single <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code> or a tuple of <code class="docutils literal notranslate"><span class="pre">tf.Tensor</span></code>. Should be valid
input to <code class="docutils literal notranslate"><span class="pre">tf.keras.losses.Loss</span></code></p></li>
<li><p><strong>eval_kwargs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – A dictionary of any additional evaluation parameters for user.
These will not be input to loss or metrics for TF, only stored in
this dataset for other custom usage.</p></li>
</ul>
</dd>
<dt class="field-even">Example<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">features</span></code> and <code class="docutils literal notranslate"><span class="pre">labels</span></code> should be in a format compatible with
this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="o">**</span><span class="n">train_kwargs</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">Loss</span><span class="p">()(</span><span class="n">labels</span><span class="p">,</span> <span class="n">preds</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="pfl.data.tensorflow.TFDataDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pfl.data.tensorflow.</span></span><span class="sig-name descname"><span class="pre">TFDataDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">raw_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefetch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metadata</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.tensorflow.TFDataDataset" title="Link to this definition">#</a></dt>
<dd><p>Dataset for representing a user using <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code>
as source. Use this if the (user) dataset is big and can’t fit
into memory, e.g. a big central eval dataset or cross-silo FL.
Otherwise, because the overhead to initialize a <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code>
is big, consider using faster in-memory dataset
<a class="reference internal" href="#pfl.data.tensorflow.TFTensorDataset" title="pfl.data.tensorflow.TFTensorDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFTensorDataset</span></code></a> along with
<a class="reference internal" href="#pfl.data.tensorflow.TFFederatedDataset" title="pfl.data.tensorflow.TFFederatedDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFFederatedDataset</span></code></a> for parallel
preprocessing.</p>
<dl class="field-list">
<dt class="field-odd">Example<span class="colon">:</span></dt>
<dd class="field-odd"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">zip</span><span class="p">(</span>
    <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span><span class="n">features</span><span class="p">),</span>
     <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span><span class="n">labels</span><span class="p">)))</span>
<span class="n">TFDataDataset</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">prefetch</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</dd>
<dt class="field-even">Parameters<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>raw_data</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code>) – The TF dataset to represent a user.</p></li>
<li><p><strong>prefetch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – How many batches to prefetch. <code class="docutils literal notranslate"><span class="pre">0</span></code> for no prefetching.</p></li>
<li><p><strong>train_kwargs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – Additional keyword arguments to propagate through pfl for the
user. Will be unpacked in the tensorflow model’s call to loss function.</p></li>
<li><p><strong>eval_kwargs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – Additional keyword arguments to propagate through pfl for the
user. Will be unpacked in the tensorflow model’s call to <code class="docutils literal notranslate"><span class="pre">metrics</span></code>
function.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.tensorflow.TFDataDataset.split">
<span class="sig-name descname"><span class="pre">split</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fraction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_train_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_val_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.tensorflow.TFDataDataset.split" title="Link to this definition">#</a></dt>
<dd><p>Split the dataset into two smaller disjoint datasets. Used by
algorithms that require both a train and val dataset for each user.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fraction</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]) – The fraction of data to split at. Defaults to <code class="docutils literal notranslate"><span class="pre">0.8</span></code></p></li>
<li><p><strong>min_train_size</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – The minimum number of samples for the train partition after
the split. If the dataset size is <code class="docutils literal notranslate"><span class="pre">4</span></code> and split is done with the
parameters <code class="docutils literal notranslate"><span class="pre">fraction=0.1</span></code> and <code class="docutils literal notranslate"><span class="pre">min_train_size=1</span></code>, the split
would result in a train set of size <code class="docutils literal notranslate"><span class="pre">0</span></code>, but <code class="docutils literal notranslate"><span class="pre">min_train_size</span></code>
overrides that to <code class="docutils literal notranslate"><span class="pre">1</span></code>.
Defaults to <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p><strong>min_val_size</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Same as <code class="docutils literal notranslate"><span class="pre">min_train_size</span></code>, but for the val partition. An error
is thrown if dataset length before split is less than
<code class="docutils literal notranslate"><span class="pre">min_train_size+min_val_size</span></code>.
Defaults to <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<a class="reference internal" href="#pfl.data.dataset.Dataset" title="pfl.data.dataset.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a>, <a class="reference internal" href="#pfl.data.dataset.Dataset" title="pfl.data.dataset.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a>]</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A tuple <code class="docutils literal notranslate"><span class="pre">(left_dataset,</span> <span class="pre">right_dataset)</span></code>, where <code class="docutils literal notranslate"><span class="pre">left_dataset</span></code>
is a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> with a fraction of <code class="docutils literal notranslate"><span class="pre">fraction</span></code> of the data
and <code class="docutils literal notranslate"><span class="pre">right_dataset</span></code> is a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> with a fraction of
<code class="docutils literal notranslate"><span class="pre">1-fraction</span></code> of the data.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.tensorflow.TFDataDataset.get_worker_partition">
<span class="sig-name descname"><span class="pre">get_worker_partition</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.tensorflow.TFDataDataset.get_worker_partition" title="Link to this definition">#</a></dt>
<dd><p>Partition the dataset among active workers in multi-worker training and
return a unique partition for the current worker.
If multi-worker training is not in use, this method just returns the
identity because the partitioning is 1 set with the full data.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#pfl.data.tensorflow.TFDataDataset" title="pfl.data.tensorflow.TFDataDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFDataDataset</span></code></a></p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A subset of this dataset, unique to the current worker. All worker
partitions add up to the original dataset.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class" id="module-pfl.data.pytorch">
<dt class="sig sig-object py" id="pfl.data.pytorch.PyTorchTensorDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pfl.data.pytorch.</span></span><span class="sig-name descname"><span class="pre">PyTorchTensorDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensors</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metadata</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_indices</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.pytorch.PyTorchTensorDataset" title="Link to this definition">#</a></dt>
<dd><p>In-memory PyTorch tensors representing a user dataset.
See <a class="reference internal" href="#pfl.data.dataset.Dataset" title="pfl.data.dataset.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a> for more information about
the class and its parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>tensors</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">...</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]]) – <p>A list or dictionary of tensors which can be accepted into your model’s
<code class="docutils literal notranslate"><span class="pre">loss</span></code> and <code class="docutils literal notranslate"><span class="pre">metrics</span></code> functions like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="o">*</span><span class="n">tensors</span><span class="p">,</span> <span class="o">**</span><span class="n">train_kwargs</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">metrics</span><span class="p">(</span><span class="o">*</span><span class="n">tensors</span><span class="p">,</span> <span class="o">**</span><span class="n">eval_kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="model.html#pfl.model.pytorch.PyTorchModel" title="pfl.model.pytorch.PyTorchModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchModel</span></code></a> for more
information about <code class="docutils literal notranslate"><span class="pre">loss</span></code> and <code class="docutils literal notranslate"><span class="pre">metrics</span></code> functions.</p>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="pfl.data.pytorch.PyTorchDataDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pfl.data.pytorch.</span></span><span class="sig-name descname"><span class="pre">PyTorchDataDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">raw_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metadata</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">dataloader_kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.pytorch.PyTorchDataDataset" title="Link to this definition">#</a></dt>
<dd><p>Dataset for representing a user using <code class="docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code>
as source. Use this if the (user) dataset is big and can’t fit
into memory, e.g. a big central eval dataset or cross-silo FL.
Otherwise, consider using faster in-memory dataset
<a class="reference internal" href="#pfl.data.dataset.Dataset" title="pfl.data.dataset.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a> along with
<a class="reference internal" href="#pfl.data.pytorch.PyTorchFederatedDataset" title="pfl.data.pytorch.PyTorchFederatedDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchFederatedDataset</span></code></a> for parallel
preprocessing.</p>
<dl class="field-list">
<dt class="field-odd">Example<span class="colon">:</span></dt>
<dd class="field-odd"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">100</span><span class="p">,))</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span>

<span class="n">PyTorchDataDataset</span><span class="p">(</span><span class="n">raw_data</span><span class="o">=</span><span class="n">MyDataset</span><span class="p">())</span>
</pre></div>
</div>
</dd>
<dt class="field-even">Parameters<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>raw_data</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code>) – The PyTorch dataset to represent a user.</p></li>
<li><p><strong>train_kwargs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – Additional keyword arguments to propagate through pfl for the
user. Will be unpacked in the PyTorch model’s call to loss function.</p></li>
<li><p><strong>eval_kwargs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – Additional keyword arguments to propagate through pfl for the
user. Will be unpacked in the PyTorch model’s call to <code class="docutils literal notranslate"><span class="pre">metrics</span></code>
function.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.pytorch.PyTorchDataDataset.split">
<span class="sig-name descname"><span class="pre">split</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fraction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_train_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_val_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.pytorch.PyTorchDataDataset.split" title="Link to this definition">#</a></dt>
<dd><p>Split the dataset into two smaller disjoint datasets. Used by
algorithms that require both a train and val dataset for each user.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fraction</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]) – The fraction of data to split at. Defaults to <code class="docutils literal notranslate"><span class="pre">0.8</span></code></p></li>
<li><p><strong>min_train_size</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – The minimum number of samples for the train partition after
the split. If the dataset size is <code class="docutils literal notranslate"><span class="pre">4</span></code> and split is done with the
parameters <code class="docutils literal notranslate"><span class="pre">fraction=0.1</span></code> and <code class="docutils literal notranslate"><span class="pre">min_train_size=1</span></code>, the split
would result in a train set of size <code class="docutils literal notranslate"><span class="pre">0</span></code>, but <code class="docutils literal notranslate"><span class="pre">min_train_size</span></code>
overrides that to <code class="docutils literal notranslate"><span class="pre">1</span></code>.
Defaults to <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p><strong>min_val_size</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Same as <code class="docutils literal notranslate"><span class="pre">min_train_size</span></code>, but for the val partition. An error
is thrown if dataset length before split is less than
<code class="docutils literal notranslate"><span class="pre">min_train_size+min_val_size</span></code>.
Defaults to <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<a class="reference internal" href="#pfl.data.dataset.Dataset" title="pfl.data.dataset.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a>, <a class="reference internal" href="#pfl.data.dataset.Dataset" title="pfl.data.dataset.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a>]</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A tuple <code class="docutils literal notranslate"><span class="pre">(left_dataset,</span> <span class="pre">right_dataset)</span></code>, where <code class="docutils literal notranslate"><span class="pre">left_dataset</span></code>
is a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> with a fraction of <code class="docutils literal notranslate"><span class="pre">fraction</span></code> of the data
and <code class="docutils literal notranslate"><span class="pre">right_dataset</span></code> is a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> with a fraction of
<code class="docutils literal notranslate"><span class="pre">1-fraction</span></code> of the data.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.pytorch.PyTorchDataDataset.get_worker_partition">
<span class="sig-name descname"><span class="pre">get_worker_partition</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.pytorch.PyTorchDataDataset.get_worker_partition" title="Link to this definition">#</a></dt>
<dd><p>Partition the dataset among active workers in multi-worker training and
return a unique partition for the current worker.
If multi-worker training is not in use, this method just returns the
identity because the partitioning is 1 set with the full data.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#pfl.data.pytorch.PyTorchDataDataset" title="pfl.data.pytorch.PyTorchDataDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchDataDataset</span></code></a></p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A subset of this dataset, unique to the current worker. All worker
partitions add up to the original dataset.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-pfl.data.federated_dataset">
<span id="federated-dataset"></span><span id="dataset"></span><h2>Federated dataset<a class="headerlink" href="#module-pfl.data.federated_dataset" title="Link to this heading">#</a></h2>
<p>A dataset is simply an iterator for iterating through datasets of individual
users.
How the user’s dataset is constructed is defined by inherited classes of
<code class="docutils literal notranslate"><span class="pre">FederatedDatasetBase</span></code>.</p>
<p>Every time <code class="docutils literal notranslate"><span class="pre">__next__</span></code> is called, a tuple <code class="docutils literal notranslate"><span class="pre">(user_data,</span> <span class="pre">seed)</span></code> is returned,
where <code class="docutils literal notranslate"><span class="pre">user_data</span></code> is a <a class="reference internal" href="#pfl.data.dataset.Dataset" title="pfl.data.dataset.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a> and <code class="docutils literal notranslate"><span class="pre">seed</span></code> is a
random integer.
The user for every call is chosen by a sampling strategy provided as argument to
the inherited subclasses of <code class="docutils literal notranslate"><span class="pre">FederatedDatasetBase</span></code>.
The random integer <code class="docutils literal notranslate"><span class="pre">seed</span></code> is different for each call, and different on each
worker, and should be used as seed for local DP to break the otherwise unwanted
behaviour of each worker generating identical noise patterns.</p>
<dl class="py class">
<dt class="sig sig-object py" id="pfl.data.federated_dataset.FederatedDatasetBase">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pfl.data.federated_dataset.</span></span><span class="sig-name descname"><span class="pre">FederatedDatasetBase</span></span><a class="headerlink" href="#pfl.data.federated_dataset.FederatedDatasetBase" title="Link to this definition">#</a></dt>
<dd><p>Base class for federated datasets used by the simulator.
A federated dataset contains many smaller subsets of data, representing a
user’s data.</p>
<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.federated_dataset.FederatedDatasetBase.get_cohort">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">get_cohort</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cohort_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.federated_dataset.FederatedDatasetBase.get_cohort" title="Link to this definition">#</a></dt>
<dd><p>Fetch an entire cohort of users. In the context of multi worker
training, only the users assigned to the current worker should be
returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>cohort_size</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The number of users to fetch.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterable</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<a class="reference internal" href="#pfl.data.dataset.AbstractDataset" title="pfl.data.dataset.AbstractDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">AbstractDataset</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>An iterable to iterate through each user dataset from the cohort.
Each step of the iterator returns a tuple <cite>(user_dataset, seed)</cite>.
Lazy loading of the users’ datasets is performed when stepping
through iterator.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="pfl.data.federated_dataset.ArtificialFederatedDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pfl.data.federated_dataset.</span></span><span class="sig-name descname"><span class="pre">ArtificialFederatedDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">make_dataset_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_sampler</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_dataset_len</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.federated_dataset.ArtificialFederatedDataset" title="Link to this definition">#</a></dt>
<dd><p>Simulates a federated dataset by automatically grouping data points into
simulated users.
This class is useful when there is no such thing as an associated user
identifier for each sample, but will of course work if you happen to have
user identifiers as well.
How a user dataset is generated is implemented by the sampling mechanism
provided.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>make_dataset_fn</strong> – <p>A function that takes as input a list of sample indices and returns the
generated user dataset.
You are expected to implement this function yourself in any way you
want, but the interface should be
<cite>func(dataset_indices) -&gt; user_dataset</cite>.
You can find example implementations in the class methods.
Note that the function to implement here differs from the function to
provide when constructing a <cite>FederatedDataset</cite>:</p>
<ul>
<li><p><cite>ArtificialFederatedDataset</cite> -
<cite>func(dataset_indices) -&gt; user_dataset</cite>.</p></li>
<li><p><cite>FederatedDataset</cite> - <cite>func(user_id) -&gt; user_dataset</cite>.</p></li>
</ul>
</p></li>
<li><p><strong>data_sampler</strong> – Sampling mechanism that samples datapoint indices to later use for
constructing a new user dataset.
The definition of a sampling mechanism is simply a callable
<cite>callable(dataset_length) -&gt; dataset_indices</cite>.
The purpose of the sampling mechanism is for you to be able to have
control over the distribution of the generated user dataset, which
should ideally be as close to how a real federated dataset would look
like for your use case.
The factory called <cite>get_data_sampler</cite> provides some examples of how this
callable might look like.</p></li>
<li><p><strong>sample_dataset_len</strong> – <p>A callable that should sample the dataset length of a user, i.e.
<cite>callable() -&gt; dataset_length</cite>.</p>
<dl class="simple">
<dt>Example:</dt><dd><p><cite>data_sampler = lambda: max(1, np.random.poisson(5)</cite>.
<cite>data_sampler</cite> is a callable (function) that draws a user dataset
size from a poisson distribution of mean 5, and should be a minimum
of size 1.</p>
</dd>
</dl>
</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.federated_dataset.ArtificialFederatedDataset.from_slices">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_slices</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_sampler</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_dataset_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">create_dataset_fn=&lt;function</span> <span class="pre">ArtificialFederatedDataset.&lt;lambda&gt;&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.federated_dataset.ArtificialFederatedDataset.from_slices" title="Link to this definition">#</a></dt>
<dd><p>Construct a simulated federated dataset from a regular dataset where
there is no such thing as a user identifier.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – A list of <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code>, i.e. the same format as a <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>
accepts.</p></li>
<li><p><strong>create_dataset_fn</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>) – A lambda function to create an instance of Dataset, or a subclass
of Dataset.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>An instance of <cite>ArtificialFederatedDataset</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.federated_dataset.ArtificialFederatedDataset.get_cohort">
<span class="sig-name descname"><span class="pre">get_cohort</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cohort_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.federated_dataset.ArtificialFederatedDataset.get_cohort" title="Link to this definition">#</a></dt>
<dd><p>Fetch an entire cohort of users. In the context of multi worker
training, only the users assigned to the current worker should be
returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>cohort_size</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The number of users to fetch.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterable</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<a class="reference internal" href="#pfl.data.dataset.AbstractDataset" title="pfl.data.dataset.AbstractDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">AbstractDataset</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>An iterable to iterate through each user dataset from the cohort.
Each step of the iterator returns a tuple <cite>(user_dataset, seed)</cite>.
Lazy loading of the users’ datasets is performed when stepping
through iterator.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="pfl.data.federated_dataset.FederatedDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pfl.data.federated_dataset.</span></span><span class="sig-name descname"><span class="pre">FederatedDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">make_dataset_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_sampler</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_id_to_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.federated_dataset.FederatedDataset" title="Link to this definition">#</a></dt>
<dd><p>A federated dataset is a collection of smaller datasets that are each
associated to a unique user.
Iterating through an instance of this class will each time return the
dataset for a specific user.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>make_dataset_fn</strong> – A function that takes as input a user identifier and returns a user
dataset.
You are expected to implement this function yourself in any way you
want, but the interface should be <cite>func(user_id) -&gt; user_dataset</cite>.
You can find example implementations in the class methods.</p></li>
<li><p><strong>user_sampler</strong> – <p>Sampling mechanism that samples a user id.
The interface of the user sampling mechanism should be
<cite>callable() -&gt; user_id</cite>.
In most cases you want to use <cite>MinimizeReuseUserSampler</cite> because its
behaviour mimics what usually happens in live federated learning with
user devices.
See <cite>MinimizeReuseUserSampler</cite> for explanation why.</p>
<p>The factory called <cite>get_user_sampler</cite> provides some examples of how this
callable might look like.</p>
</p></li>
<li><p><strong>user_id_to_weight</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]) – A dictionary mapping user id to a weight which acts as a proxy
for compute time to train this user. In most cases, when model
training time scales with data, number of user
datapoints/tokens/batches should be a good estimate.
This is solely used for minimizing straggling processes in distributed
simulations. Leaving this <code class="docutils literal notranslate"><span class="pre">None</span></code> will have same performance result
but simulations will be slower if users have varying dataset sizes.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.federated_dataset.FederatedDataset.from_slices">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_slices</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_sampler</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.federated_dataset.FederatedDataset.from_slices" title="Link to this definition">#</a></dt>
<dd><p>Construct a federated dataset from a dictionary of user to dataset
mappings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>data</strong> – A dictionary <cite>user_id:dataset</cite>, where <cite>user_id</cite> is the unique user
identifier and <cite>dataset</cite> is the dataset of the user (represented as
a list of <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> like in <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>).</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>An instance of <cite>FederatedDataset</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.federated_dataset.FederatedDataset.from_slices_with_dirichlet_class_distribution">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_slices_with_dirichlet_class_distribution</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_dataset_len_sampler</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spread_distribution_after_num_fails</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spread_distribution_after_fails_percentage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.02</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.federated_dataset.FederatedDataset.from_slices_with_dirichlet_class_distribution" title="Link to this definition">#</a></dt>
<dd><p>Create a federated dataset by partitioning <code class="docutils literal notranslate"><span class="pre">data</span></code> into artificial
users generated by
<a class="reference internal" href="#pfl.data.partition.partition_by_dirichlet_class_distribution" title="pfl.data.partition.partition_by_dirichlet_class_distribution"><code class="xref py py-func docutils literal notranslate"><span class="pre">partition_by_dirichlet_class_distribution()</span></code></a>.
See the above partition function for more information and references.
The user partitions are constructed once using all data, and unlike
a <a class="reference internal" href="#pfl.data.federated_dataset.ArtificialFederatedDataset" title="pfl.data.federated_dataset.ArtificialFederatedDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">ArtificialFederatedDataset</span></code></a>
the user partitioning remains the same throughout the training.
Sampling the generated users is done uniformly at random.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>) – A tuple of tensors, representing the data to be partitioned
(including labels). Each user will have a dataset consisting of
the same number of tensors as in <code class="docutils literal notranslate"><span class="pre">data</span></code>, but they are slices of
<code class="docutils literal notranslate"><span class="pre">data</span></code>.</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>) – A one-dimensional array of all labels (integers). Must have same
length as the first dimension of every tensor in <code class="docutils literal notranslate"><span class="pre">data</span></code>.
If <code class="docutils literal notranslate"><span class="pre">labels</span></code> should also be included as one of the tensors in
the dataset, be sure to also include it in <code class="docutils literal notranslate"><span class="pre">data</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>See
<a class="reference internal" href="#pfl.data.partition.partition_by_dirichlet_class_distribution" title="pfl.data.partition.partition_by_dirichlet_class_distribution"><code class="xref py py-func docutils literal notranslate"><span class="pre">partition_by_dirichlet_class_distribution()</span></code></a>
for further description of the parameters.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.federated_dataset.FederatedDataset.get_cohort">
<span class="sig-name descname"><span class="pre">get_cohort</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cohort_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.federated_dataset.FederatedDataset.get_cohort" title="Link to this definition">#</a></dt>
<dd><p>Fetch an entire cohort of users. In the context of multi worker
training, only the users assigned to the current worker should be
returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>cohort_size</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The number of users to fetch.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterable</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<a class="reference internal" href="#pfl.data.dataset.AbstractDataset" title="pfl.data.dataset.AbstractDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">AbstractDataset</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>An iterable to iterate through each user dataset from the cohort.
Each step of the iterator returns a tuple <cite>(user_dataset, seed)</cite>.
Lazy loading of the users’ datasets is performed when stepping
through iterator.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="pfl.data.tensorflow.TFFederatedDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pfl.data.tensorflow.</span></span><span class="sig-name descname"><span class="pre">TFFederatedDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">make_dataset_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_sampler</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_id_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_cls</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_id_to_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.tensorflow.TFFederatedDataset" title="Link to this definition">#</a></dt>
<dd><p>Create a federated dataset from a TF dataset and user sampler.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We highly encourage to use this class instead of
<a class="reference internal" href="#pfl.data.federated_dataset.FederatedDataset" title="pfl.data.federated_dataset.FederatedDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">FederatedDataset</span></code></a> when data
is loaded from disk in <cite>make_dataset_fn</cite> because using
<code class="docutils literal notranslate"><span class="pre">tf.data.Dataset.prefetch</span></code> in your code will allow users to be
loaded asynchronously into memory.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>make_dataset_fn</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code>]) – To make your <cite>tf.data.Dataset</cite> compatible with a user sampler,
this argument specifies a function that takes a <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code>
as input and returns another <code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code>. The input dataset
will return a new user id each time a step in its iterator is
taken. The output dataset should use this user id to load and
preprocess the data for an entire user, and generate a structure
that is expected by <code class="docutils literal notranslate"><span class="pre">dataset_cls</span></code>, see e.g.
<a class="reference internal" href="#pfl.data.dataset.Dataset" title="pfl.data.dataset.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a>.
To load users asynchronously, you need to have called <cite>prefetch</cite>
on the dataset that you return from <code class="docutils literal notranslate"><span class="pre">make_dataset_fn</span></code> (see the
code snippet at the end of this docstring for an example).</p></li>
<li><p><strong>user_sampler</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[], <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – A callable with no parameters that return one sampled user ID when
called.</p></li>
<li><p><strong>user_id_dtype</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DType</span></code>]) – Tensorflow data type of user ids. <code class="docutils literal notranslate"><span class="pre">tf.int32</span></code> by default.</p></li>
<li><p><strong>dataset_cls</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Type</span></code>[<a class="reference internal" href="#pfl.data.tensorflow.TFTensorDataset" title="pfl.data.tensorflow.TFTensorDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">TFTensorDataset</span></code></a>]]) – The dataset class to wrap around tensors returned from
<code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code>. This is
<a class="reference internal" href="#pfl.data.dataset.Dataset" title="pfl.data.dataset.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a> by default and doesn’t
need to be changed in most cases.</p></li>
<li><p><strong>dataset_kwargs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>]) – A dictionary for keyword arguments when constructing the pfl dataset.
If the <cite>dataset_cls</cite> is <a class="reference internal" href="#pfl.data.dataset.Dataset" title="pfl.data.dataset.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a> then the
valid keyword arguments are <cite>val_indices</cite>, <cite>train_kwargs</cite> and
<cite>eval_kwargs</cite>.</p></li>
<li><p><strong>user_id_to_weight</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]) – A dictionary mapping user id to a weight which acts as a proxy
for compute time to train this user. In most cases, when model
training time scales with data, number of user
datapoints/tokens/batches should be a good estimate.
This is solely used for minimizing straggling processes in distributed
simulations. Leaving this <code class="docutils literal notranslate"><span class="pre">None</span></code> will have same performance result
but simulations will be slower if users have varying dataset sizes.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A federated dataset where user datasets are generated from a
<code class="docutils literal notranslate"><span class="pre">tf.data.Dataset</span></code>.</p>
</dd>
<dt class="field-odd">Example<span class="colon">:</span></dt>
<dd class="field-odd"><p></p></dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Assume files with data exists.</span>
<span class="n">sample_it</span> <span class="o">=</span> <span class="n">itertools</span><span class="o">.</span><span class="n">cycle</span><span class="p">([</span><span class="s1">&#39;user1.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;user2.txt&#39;</span><span class="p">])</span>
<span class="c1"># Sampler needs to be a callable.</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="nb">next</span><span class="p">(</span><span class="n">sample_it</span><span class="p">)</span>

<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">pipeline</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="c1"># Load user dataset and do any kind of preprocessing here.</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">user_id</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">read_file</span><span class="p">(</span><span class="n">user_id</span><span class="p">))</span>
    <span class="c1"># Shuffle dataset of 1 user.</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
    <span class="c1"># Prefetch 10 users.</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data</span>

<span class="n">fed</span> <span class="o">=</span> <span class="n">FederatedDataset</span><span class="o">.</span><span class="n">from_tf_dataset</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span>
                                       <span class="n">user_id_dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="pfl.data.pytorch.PyTorchFederatedDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pfl.data.pytorch.</span></span><span class="sig-name descname"><span class="pre">PyTorchFederatedDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_sampler</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_cls</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_id_to_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">dataloader_kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.pytorch.PyTorchFederatedDataset" title="Link to this definition">#</a></dt>
<dd><p>Create a federated dataset from a PyTorch dataset and sampler.
A <cite>torch.utils.data.DataLoader</cite> is created from the arguments and is
used to load user datasets asynchronously.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code>) – A PyTorch dataset instance. <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code>, which is the method
you override to load a datapoint, should be constructed such
that the index parameter of <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code> is assumed to be a user
ID and the returned tensor should not be for one datapoint, but the
entire data of the user with this user ID. The first dimension of the
tensor(s) returned should be the number of samples for that user.
E.g., for user_id=’user_123’ who has 9 datapoints with input
features of shape [256, 256, 3] and output labels of shape [17],
dataset[‘user_123’] would consist of a list of two tensors of
shapes [torch.Size([9, 256, 256, 3]), torch.Size([9, 17])].</p></li>
<li><p><strong>user_sampler</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[], <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – Sampling mechanism that samples a user id.
The interface of the user sampling mechanism should be
<cite>callable() -&gt; user_id</cite>. This parameter is the same as
<cite>user_sampler</cite> in constructor of
:class:~`pfl.data.federated_dataset.FederatedDataset`.
In most cases you want to use <cite>MinimizeReuseUserSampler</cite> because its
behaviour mimics what usually happens in live federated learning with
user devices.</p></li>
<li><p><strong>dataset_cls</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Type</span></code>[<a class="reference internal" href="#pfl.data.pytorch.PyTorchTensorDataset" title="pfl.data.pytorch.PyTorchTensorDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchTensorDataset</span></code></a>]]) – The dataset class to wrap around tensors returned from
<code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code>. This is
<a class="reference internal" href="#pfl.data.pytorch.PyTorchTensorDataset" title="pfl.data.pytorch.PyTorchTensorDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">PyTorchTensorDataset</span></code></a> by default and doesn’t
need to be changed in most cases.</p></li>
<li><p><strong>dataset_kwargs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>]) – A dictionary for keyword arguments when constructing the pfl dataset.
If the <cite>dataset_cls</cite> is <a class="reference internal" href="#pfl.data.dataset.Dataset" title="pfl.data.dataset.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a> then the
valid keyword arguments are <cite>val_indices</cite>, <cite>train_kwargs</cite> and
<cite>eval_kwargs</cite>.</p></li>
<li><p><strong>user_id_to_weight</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]) – A dictionary mapping user id to a weight which acts as a proxy
for compute time to train this user. In most cases, when model
training time scales with data, number of user
datapoints/tokens/batches should be a good estimate.
This is solely used for minimizing straggling processes in distributed
simulations. Leaving this <code class="docutils literal notranslate"><span class="pre">None</span></code> will not affect the final outcomes
and metrics of PFL simulation, but the simulation will be slower if
users have varying dataset sizes.</p></li>
<li><p><strong>dataloader_kwargs</strong> – Keyword arguments to add to initialization of <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>.
It is important to specify these parameters correctly to receive
any kind of parallelization for data loading. For details, see
<code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="pfl.data.federated_dataset.FederatedDatasetMixture">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pfl.data.federated_dataset.</span></span><span class="sig-name descname"><span class="pre">FederatedDatasetMixture</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mixture_weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mixture_component_datasets</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.federated_dataset.FederatedDatasetMixture" title="Link to this definition">#</a></dt>
<dd><p>Simulates a mixture of federated datasets and/or artificial federated
datasets.</p>
<p>To sample new users we sample from the component datasets using the
probability vector mixture_weights. We then sample a user from the sampled
dataset.</p>
<p>The mixture of federated datasets is useful for modelling clusters or
different modes of users in a federated datasets. In particular, this class
is used for modelling a Mixture-of-Polya (Dirichlet-Multinomial)
distribution.</p>
<dl class="field-list">
<dt class="field-odd">Example<span class="colon">:</span></dt>
<dd class="field-odd"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># First component dataset is an artificial federated dataset</span>
<span class="c1"># created from data inputs X and data targets Y.</span>
<span class="n">component_dataset_0</span> <span class="o">=</span> <span class="n">ArtificialFederatedDataset</span><span class="o">.</span><span class="n">from_slices</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">],</span>
    <span class="n">data_sampler</span><span class="o">=</span><span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)),</span>
    <span class="n">sample_dataset_len</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>

<span class="c1"># Second component dataset is a federated dataset, created</span>
<span class="c1"># from a dictionary mapping user IDs to datapoints</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="p">[</span><span class="n">X_0</span><span class="p">,</span> <span class="n">y_0</span><span class="p">],</span> <span class="mi">1</span><span class="p">:</span> <span class="p">[</span><span class="n">X_1</span><span class="p">,</span> <span class="n">y_1</span><span class="p">],</span> <span class="mi">2</span><span class="p">:</span> <span class="p">[</span><span class="n">X_2</span><span class="p">,</span> <span class="n">y_2</span><span class="p">]}</span>
<span class="n">user_sampler</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">MinimizeReuseUserSampler</span><span class="p">(</span>
    <span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
<span class="n">make_user_dataset</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">user_id</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">user_id</span><span class="p">])</span>
<span class="n">component_dataset_1</span> <span class="o">=</span> <span class="n">FederatedDataset</span><span class="p">(</span><span class="n">make_user_dataset</span><span class="p">,</span>
    <span class="n">user_sampler</span><span class="p">)</span>

<span class="n">mixture_weights</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>

<span class="n">FederatedDatasetMixture</span><span class="p">(</span><span class="n">mixture_weights</span><span class="p">,</span>
                       <span class="p">[</span><span class="n">component_dataset_0</span><span class="p">,</span> <span class="n">component_dataset_1</span><span class="p">])</span>
</pre></div>
</div>
</dd>
<dt class="field-even">Parameters<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>mixture_weights</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>]) – A list or np.ndarray containing the weights for each of the component
datasets in the mixture. The mixture weights give the probability of
occurrence of each component dataset. Ideally the mixture weights sum
to 1, but if not, this class will normalise the mixture weights to sum
to 1.</p></li>
<li><p><strong>mixture_component_datasets</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<a class="reference internal" href="#pfl.data.federated_dataset.FederatedDatasetBase" title="pfl.data.federated_dataset.FederatedDatasetBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">FederatedDatasetBase</span></code></a>]) – Individual federated datasets that are the components of the mixture
of federated datasets. Each federated dataset combined form a mixture.
List of type ArtificialFederatedDataset</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.federated_dataset.FederatedDatasetMixture.get_cohort">
<span class="sig-name descname"><span class="pre">get_cohort</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cohort_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.federated_dataset.FederatedDatasetMixture.get_cohort" title="Link to this definition">#</a></dt>
<dd><p>Fetch an entire cohort of users. In the context of multi worker
training, only the users assigned to the current worker should be
returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>cohort_size</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The number of users to fetch.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterable</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<a class="reference internal" href="#pfl.data.dataset.AbstractDataset" title="pfl.data.dataset.AbstractDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">AbstractDataset</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>An iterable to iterate through each user dataset from the cohort.
Each step of the iterator returns a tuple <cite>(user_dataset, seed)</cite>.
Lazy loading of the users’ datasets is performed when stepping
through iterator.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-pfl.data.sampling">
<span id="sampling"></span><span id="get-data-sampler"></span><h2>Sampling<a class="headerlink" href="#module-pfl.data.sampling" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="pfl.data.sampling.MinimizeReuseDataSampler">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pfl.data.sampling.</span></span><span class="sig-name descname"><span class="pre">MinimizeReuseDataSampler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">max_bound</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.sampling.MinimizeReuseDataSampler" title="Link to this definition">#</a></dt>
<dd><p>Data sampling mechanism that maximises the time between instances of reuse.
This is done by simply iterating through the sample space in linear fashion
and starting over once the end is reached.
Every data sampling mechanism returns a list of indices when called.
The indices can be used to construct an artificial user dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>max_bound</strong> – Maximum bound for sampling space.
Will sample in the range <cite>[0, max_bound)</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="pfl.data.sampling.DirichletDataSampler">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pfl.data.sampling.</span></span><span class="sig-name descname"><span class="pre">DirichletDataSampler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.sampling.DirichletDataSampler" title="Link to this definition">#</a></dt>
<dd><p>Data sampling mechanism that samples user class proportions from a
Dirichlet distribution with a given alpha parameter.
Sampling is done by first drawing a vector of class proportions
p ~ Dir(alpha), then sampling a class from a categorical
distribution with parameter p and uniformly at random choosing
(with replacement) an index with the corresponding class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>) – Parameter of the Dirichlet distribution. Must be array_like and
have length equal to the number of unique classes present in labels.</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>) – A one-dimensional array of all labels (integers). This should have
length equal to the size of the corresponding dataset.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pfl.data.sampling.get_data_sampler">
<span class="sig-prename descclassname"><span class="pre">pfl.data.sampling.</span></span><span class="sig-name descname"><span class="pre">get_data_sampler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_bound</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.sampling.get_data_sampler" title="Link to this definition">#</a></dt>
<dd><p>Factory for data sampling mechanisms.</p>
<p>These samplers can be used when sampling data points for an artificial
user dataset in <cite>ArtificialFederatedDataset</cite>, by providing it as the
<cite>sampler</cite> argument.</p>
<p>Implemented samplers:
* random - Randomly sample from the range <cite>[0, max_bound)</cite>, max_bound
must be specified.
* minimize_reuse - Sample while minimizing the number of times a number is
sampled again, max_bound must be specified.
* dirichlet - Sample class proportions from a Dirichlet with given alpha
parameter and sample classes according to these proportions. Must specify
values for alpha and labels.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="pfl.data.sampling.MinimizeReuseUserSampler">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pfl.data.sampling.</span></span><span class="sig-name descname"><span class="pre">MinimizeReuseUserSampler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">user_ids</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.sampling.MinimizeReuseUserSampler" title="Link to this definition">#</a></dt>
<dd><p>User sampling mechanism that maximizes the time between instances of reuse,
similar to <cite>MinimizeReuseDataSampler</cite> but for sampling user ids.</p>
<p>In live PFL training, it is common that a user can not participate more
than one time per day for a particular use case.
In other words, the distance of a user being selected again during the day
is infinite.
This is what the sampling strategy is trying to mimic.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>user_ids</strong> – A list of user ids as the sampling space.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="pfl.data.sampling.CrossSiloUserSampler">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pfl.data.sampling.</span></span><span class="sig-name descname"><span class="pre">CrossSiloUserSampler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sampling_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'random'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">silo_to_user_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_silos</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.sampling.CrossSiloUserSampler" title="Link to this definition">#</a></dt>
<dd><p>User sampling mechanism that assumes the users are partitioned into
disjoint subsets belonging to different silos. In each round, all silos
contribute to the cohort and each silo may sample one or more users.</p>
<p>In the distributed setting, silos will be split into different nodes.
In each sampling call, a silo is picked according to a fixed order and then
a user is sampled from the subset in that silo.</p>
<dl class="field-list">
<dt class="field-odd">Example<span class="colon">:</span></dt>
<dd class="field-odd"><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">silo_to_user_ids</span> <span class="o">=</span> <span class="p">{</span>
    <span class="mi">0</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="mi">1</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="mi">2</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
    <span class="mi">3</span><span class="p">:</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="p">}</span>
<span class="n">num_silos</span> <span class="o">=</span> <span class="mi">4</span>
<span class="c1"># Assume 4 processes on 2 nodes</span>
<span class="c1"># Node 1 will hold silo 0 and 1</span>
<span class="c1"># Node 2 will hold silos 2 and 3</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="n">CrossSiloUserSampler</span><span class="p">(</span>
    <span class="n">sampling_type</span><span class="o">=</span><span class="s1">&#39;minimize_reuse&#39;</span><span class="p">,</span>
    <span class="n">silo_to_user_ids</span><span class="o">=</span><span class="n">silo_to_user_ids</span><span class="p">)</span>
<span class="p">[</span><span class="n">sampler</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span>
</pre></div>
</div>
</dd>
<dt class="field-even">Parameters<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>sampling_type</strong> – The sampling method used to choose a user from a silo.</p></li>
<li><p><strong>user_ids</strong> – A list of user ids as the sampling space.</p></li>
<li><p><strong>silo_to_user_ids</strong> – An optional dictionary where the keys are the silos and the values are
the corresponding subset of user ids.
If not provided, the <cite>user_ids</cite> will be evenly split into <cite>n</cite> silos
where <cite>n</cite> is the number of workers in the distributed setting.</p></li>
<li><p><strong>num_silos</strong> – An optional int for indicating number of silos to partition the users
if <cite>silo_to_user_ids</cite> is not provided.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pfl.data.sampling.get_user_sampler">
<span class="sig-prename descclassname"><span class="pre">pfl.data.sampling.</span></span><span class="sig-name descname"><span class="pre">get_user_sampler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_ids</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">silo_to_user_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_silos</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.sampling.get_user_sampler" title="Link to this definition">#</a></dt>
<dd><p>Factory for user sampling mechanisms.</p>
<p>These can be used when sampling the next user for <cite>FederatedDataset</cite>
by providing it as the <cite>user_sampler</cite> argument.</p>
<p>Implemented samplers:
* random - Randomly sample a user id.
* minimize_reuse - Sample a user while minimizing the number of times the
user is sampled again.</p>
</dd></dl>

</section>
<section id="module-pfl.data.partition">
<span id="partitioning"></span><h2>Partitioning<a class="headerlink" href="#module-pfl.data.partition" title="Link to this heading">#</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="pfl.data.partition.partition_by_dirichlet_class_distribution">
<span class="sig-prename descclassname"><span class="pre">pfl.data.partition.</span></span><span class="sig-name descname"><span class="pre">partition_by_dirichlet_class_distribution</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">labels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_dataset_len_sampler</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spread_distribution_after_num_fails</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spread_distribution_after_fails_percentage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.02</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.partition.partition_by_dirichlet_class_distribution" title="Link to this definition">#</a></dt>
<dd><p>Given an array of labels, create a partitioning representing artificial user
dataset splits where each user’s class distribution is sampled ~Dir(alpha).</p>
<p>It is common to use <cite>alpha=0.1</cite> (majority of samples for each user will be
from 1 class). See S.J. Reddi et al. <a class="reference external" href="https://arxiv.org/pdf/2003.00295.pdf">https://arxiv.org/pdf/2003.00295.pdf</a>,
J. Wang. et al <a class="reference external" href="https://arxiv.org/pdf/2007.07481.pdf">https://arxiv.org/pdf/2007.07481.pdf</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labels</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>) – A one-dimensional array of all labels (integers). Classes should be
consecutive non-negative integers starting at 0, i.e. from
{0, 1, …, num_classes-1}.</p></li>
<li><p><strong>alpha</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – The alpha parameter for Dirichlet distribution.</p></li>
<li><p><strong>user_dataset_len_sampler</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>) – A function which samples the dataset length of the user to construct
next. E.g., use <code class="docutils literal notranslate"><span class="pre">lambda:</span> <span class="pre">25</span></code> to sample 25 data points for all users.</p></li>
<li><p><strong>spread_distribution_after_num_fails</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – When there are few datapoints left to sample, there might not be any
datapoints left to sample for particular classes.
Each (user_num_datapoints * spread_distribution_after_num_fails)
iterations of sampling each user, even out the class distribution by
adding <cite>spread_distribution_after_fails_percentage</cite> to each class
probability and normalize (this will make the distribution less
heterogeneous and move it closer to the uniform one).
This will normally only start occurring when there are only a few
datapoints left to sample from.</p></li>
<li><p><strong>spread_distribution_after_fails_percentage</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>) – See above how this works with <code class="docutils literal notranslate"><span class="pre">spread_distribution_after_num_fails</span></code>.
E.g., the default value 0.02 increases each class probability by
2% and then renormalizes to ensure that the probabilities sum to 1.
This moves the distribution closer to the uniform one. E.g., for 2
classes with probabilities [0.1, 0.9] we end up adjusting to
[0.12, 0.92] first and then normalize to [0.11538462, 0.88461538].</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A list, where each element is a list of <code class="docutils literal notranslate"><span class="pre">label</span></code> indices that
represents one sampled user.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-pfl.data.user_state">
<span id="user-state"></span><h2>User state<a class="headerlink" href="#module-pfl.data.user_state" title="Link to this heading">#</a></h2>
<p>Sometimes, Federated Learning algorithms require users to be stateful,
e.g. each user might have unique parameter values that are used and updated
only locally. This module implements various methods of saving and
retrieving such user state.</p>
<dl class="py class">
<dt class="sig sig-object py" id="pfl.data.user_state.AbstractUserStateStorage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pfl.data.user_state.</span></span><span class="sig-name descname"><span class="pre">AbstractUserStateStorage</span></span><a class="headerlink" href="#pfl.data.user_state.AbstractUserStateStorage" title="Link to this definition">#</a></dt>
<dd><p>Abstract base class for storage to save and load
user state.</p>
<p>Incorporating user state in a custom algorithm might look something like
this:</p>
<dl class="field-list">
<dt class="field-odd">Example<span class="colon">:</span></dt>
<dd class="field-odd"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">storage</span> <span class="o">=</span> <span class="n">InMemoryUserStateStorage</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">MyAlgorithm</span><span class="p">(</span><span class="n">FederatedNNAlgorithm</span><span class="p">):</span>

  <span class="k">def</span> <span class="nf">train_one_user</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">initial_model_state</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">user_dataset</span><span class="p">,</span>
                     <span class="n">central_context</span><span class="p">):</span>
    <span class="n">user_state</span> <span class="o">=</span> <span class="n">storage</span><span class="o">.</span><span class="n">load_state</span><span class="p">(</span><span class="n">user_dataset</span><span class="o">.</span><span class="n">user_id</span><span class="p">,</span> <span class="s1">&#39;my-algo&#39;</span><span class="p">)</span>
    <span class="c1"># ... do local algorithm here, which also generates a new state</span>
    <span class="n">storage</span><span class="o">.</span><span class="n">save_state</span><span class="p">(</span><span class="n">new_user_state</span><span class="p">,</span> <span class="n">user_dataset</span><span class="o">.</span><span class="n">user_id</span><span class="p">,</span> <span class="s1">&#39;my-algo&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">get_model_difference</span><span class="p">(</span><span class="n">initial_model_state</span><span class="p">),</span> <span class="n">Metrics</span><span class="p">()</span>
</pre></div>
</div>
</dd>
</dl>
<p>If the user state is particularly big, e.g. a full set of model weights,
loading the state in the data loading stage can be beneficial if the data
loading stage is already parallelized.</p>
<dl class="field-list">
<dt class="field-odd">Example<span class="colon">:</span></dt>
<dd class="field-odd"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">storage</span> <span class="o">=</span> <span class="n">InMemoryUserStateStorage</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">make_dataset_fn</span><span class="p">(</span><span class="n">user_id</span><span class="p">):</span>
    <span class="n">raw_data</span> <span class="o">=</span> <span class="n">load_raw_data</span><span class="p">(</span><span class="n">user_id</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Dataset</span><span class="p">(</span>
        <span class="n">raw_data</span><span class="o">=</span><span class="p">[</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">],</span>
        <span class="n">local_state</span><span class="o">=</span><span class="n">storage</span><span class="o">.</span><span class="n">load_state</span><span class="p">(</span><span class="n">user_id</span><span class="p">,</span> <span class="s1">&#39;my-algo&#39;</span><span class="p">))</span>

<span class="c1"># fed_data is input to SimulatedAggregator</span>
<span class="n">fed_data</span> <span class="o">=</span> <span class="n">FederatedDataset</span><span class="p">(</span>
    <span class="n">make_dataset_fn</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span> <span class="n">user_id_to_weight</span><span class="o">=</span><span class="n">user_num_images</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">MyAlgorithm</span><span class="p">(</span><span class="n">FederatedNNAlgorithm</span><span class="p">):</span>

  <span class="k">def</span> <span class="nf">train_one_user</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">initial_model_state</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">user_dataset</span><span class="p">,</span>
                     <span class="n">central_context</span><span class="p">):</span>
    <span class="n">user_state</span> <span class="o">=</span> <span class="n">user_dataset</span><span class="o">.</span><span class="n">local_state</span>
    <span class="c1"># ... do local algorithm here, which also generate a new state</span>
    <span class="n">storage</span><span class="o">.</span><span class="n">save_state</span><span class="p">(</span><span class="n">new_user_state</span><span class="p">,</span> <span class="n">user_dataset</span><span class="o">.</span><span class="n">user_id</span><span class="p">,</span> <span class="s1">&#39;my-algo&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">get_model_difference</span><span class="p">(</span><span class="n">initial_model_state</span><span class="p">),</span> <span class="n">Metrics</span><span class="p">()</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.user_state.AbstractUserStateStorage.clear_states">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">clear_states</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.user_state.AbstractUserStateStorage.clear_states" title="Link to this definition">#</a></dt>
<dd><p>Remove all existing stored user states.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.user_state.AbstractUserStateStorage.save_state">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">save_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_id</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.user_state.AbstractUserStateStorage.save_state" title="Link to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – A dictionary with generic values to store as state.</p></li>
<li><p><strong>user_id</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – ID of the user of this state.</p></li>
<li><p><strong>key</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Multiple states of same user can be stored simultaneously if unique
keys are specified for each different state object.
Not specifying a key will overwrite the default saved user state.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.user_state.AbstractUserStateStorage.load_state">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">load_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">user_id</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.user_state.AbstractUserStateStorage.load_state" title="Link to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>user_id</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Restore the state of the user with this ID.</p></li>
<li><p><strong>key</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Optional key to load a specific user state that was previously
saved with this key. Not specifying a key will load the default
saved user state.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The user state.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="pfl.data.user_state.InMemoryUserStateStorage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pfl.data.user_state.</span></span><span class="sig-name descname"><span class="pre">InMemoryUserStateStorage</span></span><a class="headerlink" href="#pfl.data.user_state.InMemoryUserStateStorage" title="Link to this definition">#</a></dt>
<dd><p>Save and load user state for a given user ID.
Keeps states of all users in memory such that loading
and saving state is very fast.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Saving user state in memory is neither compatible out of the box with
distributed simulations on multiple machines nor on multiple processes
on same machine. If your large simulations require multiple processes,
you can use a cross-silo user sampler such that a unique user is pinned
to being sampled in the same worker process each time, where the state
of that user is cached.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.user_state.InMemoryUserStateStorage.clear_states">
<span class="sig-name descname"><span class="pre">clear_states</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.user_state.InMemoryUserStateStorage.clear_states" title="Link to this definition">#</a></dt>
<dd><p>Remove all existing stored user states.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.user_state.InMemoryUserStateStorage.save_state">
<span class="sig-name descname"><span class="pre">save_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_id</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.user_state.InMemoryUserStateStorage.save_state" title="Link to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – A dictionary with generic values to store as state.</p></li>
<li><p><strong>user_id</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – ID of the user of this state.</p></li>
<li><p><strong>key</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Multiple states of same user can be stored simultaneously if unique
keys are specified for each different state object.
Not specifying a key will overwrite the default saved user state.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.user_state.InMemoryUserStateStorage.load_state">
<span class="sig-name descname"><span class="pre">load_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">user_id</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.user_state.InMemoryUserStateStorage.load_state" title="Link to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>user_id</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Restore the state of the user with this ID.</p></li>
<li><p><strong>key</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Optional key to load a specific user state that was previously
saved with this key. Not specifying a key will load the default
saved user state.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The user state.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="pfl.data.user_state.DiskUserStateStorage">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pfl.data.user_state.</span></span><span class="sig-name descname"><span class="pre">DiskUserStateStorage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dir_path</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.user_state.DiskUserStateStorage" title="Link to this definition">#</a></dt>
<dd><p>Save and load user state for a given user ID.
Keeps states of all users on disk.
This is slower than InMemoryUserStateStorage, but
necessary if the user states are too large to fit
into memory.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Saving user state on disk is not compatible out of the
box with distributed simulations on multiple machines.
Try to use a single machine with enough GPUs and
a common disk space. If your large simulations
require multiple machines, you can use a cross-silo
user sampler such that a unique user is pinned
to being sampled on the same machine each time.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.user_state.DiskUserStateStorage.clear_states">
<span class="sig-name descname"><span class="pre">clear_states</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.user_state.DiskUserStateStorage.clear_states" title="Link to this definition">#</a></dt>
<dd><p>Remove all existing stored user states.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.user_state.DiskUserStateStorage.acquire_lock">
<span class="sig-name descname"><span class="pre">acquire_lock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.user_state.DiskUserStateStorage.acquire_lock" title="Link to this definition">#</a></dt>
<dd><p>Acquire lock on a specific file.
The lock state is written to disk and thereby works
across processes that share disk space.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>path</strong> – Acquire lock for this file.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.user_state.DiskUserStateStorage.save_state">
<span class="sig-name descname"><span class="pre">save_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_id</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.user_state.DiskUserStateStorage.save_state" title="Link to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – A dictionary with generic values to store as state.</p></li>
<li><p><strong>user_id</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – ID of the user of this state.</p></li>
<li><p><strong>key</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Multiple states of same user can be stored simultaneously if unique
keys are specified for each different state object.
Not specifying a key will overwrite the default saved user state.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pfl.data.user_state.DiskUserStateStorage.load_state">
<span class="sig-name descname"><span class="pre">load_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">user_id</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pfl.data.user_state.DiskUserStateStorage.load_state" title="Link to this definition">#</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>user_id</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – Restore the state of the user with this ID.</p></li>
<li><p><strong>key</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Optional key to load a specific user state that was previously
saved with this key. Not specifying a key will load the default
saved user state.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The user state.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="exception.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Exception</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="context.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Context</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2024 Apple Inc.
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            <div class="last-updated">
              Last updated on May 28, 2025</div>
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Data</a><ul>
<li><a class="reference internal" href="#module-pfl.data.dataset">User dataset</a><ul>
<li><a class="reference internal" href="#pfl.data.dataset.AbstractDataset"><code class="docutils literal notranslate"><span class="pre">AbstractDataset</span></code></a><ul>
<li><a class="reference internal" href="#pfl.data.dataset.AbstractDataset.split"><code class="docutils literal notranslate"><span class="pre">AbstractDataset.split()</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.dataset.AbstractDataset.get_worker_partition"><code class="docutils literal notranslate"><span class="pre">AbstractDataset.get_worker_partition()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#pfl.data.dataset.Dataset"><code class="docutils literal notranslate"><span class="pre">Dataset</span></code></a><ul>
<li><a class="reference internal" href="#pfl.data.dataset.Dataset.split"><code class="docutils literal notranslate"><span class="pre">Dataset.split()</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.dataset.Dataset.get_worker_partition"><code class="docutils literal notranslate"><span class="pre">Dataset.get_worker_partition()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#pfl.data.dataset.TabularDataset"><code class="docutils literal notranslate"><span class="pre">TabularDataset</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.dataset.DatasetSplit"><code class="docutils literal notranslate"><span class="pre">DatasetSplit</span></code></a><ul>
<li><a class="reference internal" href="#pfl.data.dataset.DatasetSplit.raw_data"><code class="docutils literal notranslate"><span class="pre">DatasetSplit.raw_data</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.dataset.DatasetSplit.train_kwargs"><code class="docutils literal notranslate"><span class="pre">DatasetSplit.train_kwargs</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.dataset.DatasetSplit.eval_kwargs"><code class="docutils literal notranslate"><span class="pre">DatasetSplit.eval_kwargs</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.dataset.DatasetSplit.split"><code class="docutils literal notranslate"><span class="pre">DatasetSplit.split()</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.dataset.DatasetSplit.get_worker_partition"><code class="docutils literal notranslate"><span class="pre">DatasetSplit.get_worker_partition()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#pfl.data.tensorflow.TFTensorDataset"><code class="docutils literal notranslate"><span class="pre">TFTensorDataset</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.tensorflow.TFDataDataset"><code class="docutils literal notranslate"><span class="pre">TFDataDataset</span></code></a><ul>
<li><a class="reference internal" href="#pfl.data.tensorflow.TFDataDataset.split"><code class="docutils literal notranslate"><span class="pre">TFDataDataset.split()</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.tensorflow.TFDataDataset.get_worker_partition"><code class="docutils literal notranslate"><span class="pre">TFDataDataset.get_worker_partition()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#pfl.data.pytorch.PyTorchTensorDataset"><code class="docutils literal notranslate"><span class="pre">PyTorchTensorDataset</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.pytorch.PyTorchDataDataset"><code class="docutils literal notranslate"><span class="pre">PyTorchDataDataset</span></code></a><ul>
<li><a class="reference internal" href="#pfl.data.pytorch.PyTorchDataDataset.split"><code class="docutils literal notranslate"><span class="pre">PyTorchDataDataset.split()</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.pytorch.PyTorchDataDataset.get_worker_partition"><code class="docutils literal notranslate"><span class="pre">PyTorchDataDataset.get_worker_partition()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pfl.data.federated_dataset">Federated dataset</a><ul>
<li><a class="reference internal" href="#pfl.data.federated_dataset.FederatedDatasetBase"><code class="docutils literal notranslate"><span class="pre">FederatedDatasetBase</span></code></a><ul>
<li><a class="reference internal" href="#pfl.data.federated_dataset.FederatedDatasetBase.get_cohort"><code class="docutils literal notranslate"><span class="pre">FederatedDatasetBase.get_cohort()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#pfl.data.federated_dataset.ArtificialFederatedDataset"><code class="docutils literal notranslate"><span class="pre">ArtificialFederatedDataset</span></code></a><ul>
<li><a class="reference internal" href="#pfl.data.federated_dataset.ArtificialFederatedDataset.from_slices"><code class="docutils literal notranslate"><span class="pre">ArtificialFederatedDataset.from_slices()</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.federated_dataset.ArtificialFederatedDataset.get_cohort"><code class="docutils literal notranslate"><span class="pre">ArtificialFederatedDataset.get_cohort()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#pfl.data.federated_dataset.FederatedDataset"><code class="docutils literal notranslate"><span class="pre">FederatedDataset</span></code></a><ul>
<li><a class="reference internal" href="#pfl.data.federated_dataset.FederatedDataset.from_slices"><code class="docutils literal notranslate"><span class="pre">FederatedDataset.from_slices()</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.federated_dataset.FederatedDataset.from_slices_with_dirichlet_class_distribution"><code class="docutils literal notranslate"><span class="pre">FederatedDataset.from_slices_with_dirichlet_class_distribution()</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.federated_dataset.FederatedDataset.get_cohort"><code class="docutils literal notranslate"><span class="pre">FederatedDataset.get_cohort()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#pfl.data.tensorflow.TFFederatedDataset"><code class="docutils literal notranslate"><span class="pre">TFFederatedDataset</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.pytorch.PyTorchFederatedDataset"><code class="docutils literal notranslate"><span class="pre">PyTorchFederatedDataset</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.federated_dataset.FederatedDatasetMixture"><code class="docutils literal notranslate"><span class="pre">FederatedDatasetMixture</span></code></a><ul>
<li><a class="reference internal" href="#pfl.data.federated_dataset.FederatedDatasetMixture.get_cohort"><code class="docutils literal notranslate"><span class="pre">FederatedDatasetMixture.get_cohort()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-pfl.data.sampling">Sampling</a><ul>
<li><a class="reference internal" href="#pfl.data.sampling.MinimizeReuseDataSampler"><code class="docutils literal notranslate"><span class="pre">MinimizeReuseDataSampler</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.sampling.DirichletDataSampler"><code class="docutils literal notranslate"><span class="pre">DirichletDataSampler</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.sampling.get_data_sampler"><code class="docutils literal notranslate"><span class="pre">get_data_sampler()</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.sampling.MinimizeReuseUserSampler"><code class="docutils literal notranslate"><span class="pre">MinimizeReuseUserSampler</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.sampling.CrossSiloUserSampler"><code class="docutils literal notranslate"><span class="pre">CrossSiloUserSampler</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.sampling.get_user_sampler"><code class="docutils literal notranslate"><span class="pre">get_user_sampler()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-pfl.data.partition">Partitioning</a><ul>
<li><a class="reference internal" href="#pfl.data.partition.partition_by_dirichlet_class_distribution"><code class="docutils literal notranslate"><span class="pre">partition_by_dirichlet_class_distribution()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-pfl.data.user_state">User state</a><ul>
<li><a class="reference internal" href="#pfl.data.user_state.AbstractUserStateStorage"><code class="docutils literal notranslate"><span class="pre">AbstractUserStateStorage</span></code></a><ul>
<li><a class="reference internal" href="#pfl.data.user_state.AbstractUserStateStorage.clear_states"><code class="docutils literal notranslate"><span class="pre">AbstractUserStateStorage.clear_states()</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.user_state.AbstractUserStateStorage.save_state"><code class="docutils literal notranslate"><span class="pre">AbstractUserStateStorage.save_state()</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.user_state.AbstractUserStateStorage.load_state"><code class="docutils literal notranslate"><span class="pre">AbstractUserStateStorage.load_state()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#pfl.data.user_state.InMemoryUserStateStorage"><code class="docutils literal notranslate"><span class="pre">InMemoryUserStateStorage</span></code></a><ul>
<li><a class="reference internal" href="#pfl.data.user_state.InMemoryUserStateStorage.clear_states"><code class="docutils literal notranslate"><span class="pre">InMemoryUserStateStorage.clear_states()</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.user_state.InMemoryUserStateStorage.save_state"><code class="docutils literal notranslate"><span class="pre">InMemoryUserStateStorage.save_state()</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.user_state.InMemoryUserStateStorage.load_state"><code class="docutils literal notranslate"><span class="pre">InMemoryUserStateStorage.load_state()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#pfl.data.user_state.DiskUserStateStorage"><code class="docutils literal notranslate"><span class="pre">DiskUserStateStorage</span></code></a><ul>
<li><a class="reference internal" href="#pfl.data.user_state.DiskUserStateStorage.clear_states"><code class="docutils literal notranslate"><span class="pre">DiskUserStateStorage.clear_states()</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.user_state.DiskUserStateStorage.acquire_lock"><code class="docutils literal notranslate"><span class="pre">DiskUserStateStorage.acquire_lock()</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.user_state.DiskUserStateStorage.save_state"><code class="docutils literal notranslate"><span class="pre">DiskUserStateStorage.save_state()</span></code></a></li>
<li><a class="reference internal" href="#pfl.data.user_state.DiskUserStateStorage.load_state"><code class="docutils literal notranslate"><span class="pre">DiskUserStateStorage.load_state()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=4621528c"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=32e29ea5"></script>
    </body>
</html>