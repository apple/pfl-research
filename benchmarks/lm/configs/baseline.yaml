data_path: ./data/stackoverflow/stackoverflow.hdf5
# Which dataset to train on: {reddit, stackoverflow}.
dataset: stackoverflow
# Which model to train: {lm_transformer, lm_lstm}.
model_name: lm_transformer
# Which algorithm to train with: {fedavg,fedprox}.
algorithm_name: fedavg
embedding_size: 96
# Transformer related parameters
hidden_size: 96
num_heads: 8
feedforward_size: 1536
num_transformer_layer: 3
dropout_rate: 0.1
central_lr_num_warmup_iterations: 50
# LSTM related parameters
#  - num_cell_states: 256
#  - num_lstm_layers: 1

central_optimizer: adam
adaptivity_degree: 0.01
data_fraction: 1.0
central_data_fraction: 0.01
evaluation_frequency: 20
# Stackoverflow related parameters (Wang et al. 2021 uses 64)
max_user_sentences: 64
# Reddit related parameters (McMahan et al. 2018 uses 1600)
max_user_tokens: 1600
# McMahan et al. uses max_user_tokens/max_sequence_length 
minimum_num_datapoints_per_user: 1

learning_rate: 0.1
cohort_size: 400
noise_cohort_size: 5000
# Faster simulation. Central val is used.
val_cohort_size: 0 
central_num_iterations: 2000
local_batch_size: 16
local_num_epochs: 1
local_learning_rate: 0.3
weighting: user
# TODO: rdar://109165296 Implement LocalLRDecay as an adaptive hyperparameter
local_lr_decay: False
# TODO: rdar://109165050 Implement DecayToFedSGD as an adaptive hyperparameter
# - fedsgd_after_amount_trained: 0.75
central_eval_batch_size: 1024

central_privacy_mechanism: none
central_epsilon: 2.0
central_delta: 1e-6
central_privacy_clipping_bound: 1.0
central_order: 2
population: 1e6
use_tensorboard: False
#save_model_path: './checkpoints'

#wandb_project_id: testing
# This result in all algorithm parameters being added, even though
# you only select 1 algorithm. Useful for reusing the same config
# for multiple algorithms.
add_all_arguments: true

# FedProx params from benchmark-algos/sweep-fedprox
mu: 0.001

# AdaFedProx override default params
adafedprox_metric_name: "Central val | loss"
adafedprox_adapt_frequency: 20

# SCAFFOLD override default params
scaffold_population: 342477
