# Copyright Â© 2024 Apple Inc.
import argparse
import json
import logging
import sys
from collections import Counter, defaultdict
from typing import Counter as TCounter

import h5py
import numpy as np
import tqdm
from datasets import load_dataset

logger = logging.getLogger(name=__name__)

LABEL_DELIMITER = '|'  # Labels will be joined by delimiter and saved to hdf5
LOG_INTERVAL = 100  # Log the preprocessing progress every interval steps


def load_image_from_huggingface(dataset, image_id):
    """
    Load an image from the HuggingFace dataset by image_id.

    :param dataset:
        The loaded HuggingFace dataset.
    :param image_id:
        The image_id of the image to be loaded.
    :return:
        The loaded image as a PIL Image object.
    """
    # Assuming image_id is unique and using filter to get the specific image
    image_data = dataset.filter(lambda x: x['image_id'] == image_id)
    image_entry = next(iter(image_data))  # Get the first (and only) entry
    return image_entry['image']


def preprocess_federated_dataset(output_file: str):
    """
    Process images and labels into a HDF5 federated dataset where data is
    first split by train/test partitions and then split again by user ID.

    :param dataset:
        The loaded HuggingFace dataset.
    :param output_file:
        Output path for HDF5 file. Use the postfix `.hdf5`.
    """

    # Load dataset from HuggingFace
    # This is a Dict[str, Dataset] where key is the split.
    dataset_splits = load_dataset('apple/flair', keep_in_memory=False)
    logger.info(
        f'Preprocessing federated dataset, sample record: {next(iter(dataset_splits["train"]))}'
    )

    label_counter: TCounter[str] = Counter()
    fine_grained_label_counter: TCounter[str] = Counter()

    partition_to_user_to_ix: defaultdict = defaultdict(
        lambda: defaultdict(list))
    for partition, ds in dataset_splits.items():
        for i, entry in tqdm.tqdm(
                enumerate(ds),
                total=len(ds),
                desc=f'{partition} - Mapping users to datapoints.'):
            # Make user to datapoints mapping.
            partition_to_user_to_ix[partition][entry['user_id']].append(i)
            label_counter.update(entry["labels"])
            fine_grained_label_counter.update(entry["fine_grained_labels"])

    label_to_index = {
        label: index
        for index, label in enumerate(sorted(label_counter.keys()))
    }
    fine_grained_label_to_index = {
        fine_grained_label: index
        for index, fine_grained_label in enumerate(
            sorted(fine_grained_label_counter.keys()))
    }

    with h5py.File(output_file, 'w') as h5file:
        for partition, user_to_ix in partition_to_user_to_ix.items():
            ds = dataset_splits[partition]

            # Iterate through users of each partition.
            for i, (user_id, data_indices) in tqdm.tqdm(
                    enumerate(user_to_ix.items()),
                    total=len(user_to_ix),
                    desc=f'{partition} - constructing dataset'):
                # Load and concatenate all images and labels of a user.
                image_array, image_id_array = [], []
                labels_row, labels_col = [], []
                fine_grained_labels_row, fine_grained_labels_col = [], []
                for j, data_index in enumerate(data_indices):
                    metadata = ds[data_index]
                    image_id = metadata["image_id"]
                    image_array.append(np.asarray(metadata["image"]))
                    image_id_array.append(image_id)
                    # Encode labels as row indices and column indices
                    labels_row.extend([j] * len(metadata["labels"]))
                    labels_col.extend(
                        [label_to_index[l] for l in metadata["labels"]])
                    fine_grained_labels_row.extend(
                        [j] * len(metadata["fine_grained_labels"]))
                    fine_grained_labels_col.extend([
                        fine_grained_label_to_index[l]
                        for l in metadata["fine_grained_labels"]
                    ])
                    # Update label counter
                    label_counter.update(metadata["labels"])
                    fine_grained_label_counter.update(
                        metadata["fine_grained_labels"])

                # Multiple variable-length labels. Needs to be stored as a string.
                h5file[f'/{partition}/{user_id}/labels_row'] = np.asarray(
                    labels_row, dtype=np.uint16)
                h5file[f'/{partition}/{user_id}/labels_col'] = np.asarray(
                    labels_col, dtype=np.uint8)
                h5file[
                    f'/{partition}/{user_id}/fine_grained_labels_row'] = np.asarray(
                        fine_grained_labels_row, dtype=np.uint16)
                h5file[
                    f'/{partition}/{user_id}/fine_grained_labels_col'] = np.asarray(
                        fine_grained_labels_col, dtype=np.uint16)
                h5file[f'/{partition}/{user_id}/image_ids'] = np.asarray(
                    image_id_array, dtype='S')
                # Tensor with dimensions [num_images,width,height,channels]
                h5file.create_dataset(f'/{partition}/{user_id}/images',
                                      data=np.stack(image_array))

                if (i + 1) % LOG_INTERVAL == 0:
                    logger.info(f"Processed {i + 1}/{len(user_to_ix)} users")

        # Write metadata
        h5file['/metadata/label_mapping'] = json.dumps(label_to_index)
        h5file['/metadata/fine_grained_label_mapping'] = json.dumps(
            fine_grained_label_to_index)

    logger.info('Finished preprocess federated dataset successfully!')


def preprocess_central_dataset(output_file: str):
    """
    Process images and labels into a HDF5 (not federated) dataset where
    data is split by train/val/test partitions.

    :param dataset:
        The loaded HuggingFace dataset.
    :param output_file:
        Output path for HDF5 file. Use the postfix `.hdf5`.
    """
    logger.info('Preprocessing central dataset.')
    dataset_splits = load_dataset('apple/flair', keep_in_memory=False)

    label_counter: TCounter[str] = Counter()
    fine_grained_label_counter: TCounter[str] = Counter()
    with h5py.File(output_file, 'w') as h5file:
        # Iterate through dataset.
        for partition, dataset in dataset_splits.items():
            for i, entry in tqdm.tqdm(enumerate(dataset), total=len(dataset)):
                image_id = entry["image_id"]
                image = np.asarray(
                    entry["image"]
                )  # Directly use the image array from the dataset
                h5file.create_dataset(f'/{partition}/{image_id}/image',
                                      data=image)
                # Encode labels as a single string, separated by delimiter |
                h5file[
                    f'/{partition}/{image_id}/labels'] = LABEL_DELIMITER.join(
                        entry["labels"])
                h5file[f'/{partition}/{image_id}/fine_grained_labels'] = (
                    LABEL_DELIMITER.join(entry["fine_grained_labels"]))
                h5file[f'/{partition}/{image_id}/user_id'] = entry["user_id"]
                # Update label counter
                label_counter.update(entry["labels"])
                fine_grained_label_counter.update(entry["fine_grained_labels"])

                if (i + 1) % LOG_INTERVAL == 0:
                    logger.info(f"Processed {i + 1}/{len(dataset)} entries")

        # Write metadata
        h5file['/metadata/label_mapping'] = json.dumps(dict(label_counter))
        h5file['/metadata/fine_grained_label_mapping'] = json.dumps(
            dict(fine_grained_label_counter))

    logger.info('Finished preprocessing central dataset successfully!')


if __name__ == '__main__':
    logging.basicConfig(stream=sys.stdout,
                        level=logging.INFO,
                        format='%(asctime)s %(levelname)s: %(message)s')

    argument_parser = argparse.ArgumentParser(
        description=
        'Download and preprocess the images and labels of FLAIR dataset into HDF5 files.'
    )
    argument_parser.add_argument(
        '--output_file',
        required=True,
        help='Path to output HDF5 file that will be constructed by this script'
    )
    argument_parser.add_argument('--not_group_data_by_user',
                                 action='store_true',
                                 default=False,
                                 help='If true, do not group data by user IDs.'
                                 'If false, group data by user IDs to '
                                 'make suitable for federated learning.')
    arguments = argument_parser.parse_args()

    if arguments.not_group_data_by_user:
        preprocess_central_dataset(arguments.output_file)
    else:
        preprocess_federated_dataset(arguments.output_file)
